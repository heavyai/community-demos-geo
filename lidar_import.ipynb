{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taming the LIDAR Beast, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Dr. Michael Flaxman, Geodesign Technologies<p>\n",
    "version 1: July 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to pre-process LIDAR files in common LAS/LAZ formats into CSV files suitable for MapD import.  This includes enrichment of the data with several useful calculated attributes.  A blog post is available giving further background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-12T05:47:16.273Z"
    }
   },
   "outputs": [],
   "source": [
    "<img src=\"img/pdal_logo.png\",width=60,height=60>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make extensive use of the excellent open source \"point data abstraction library\" or PDAL.  For full documentation of this tool, please see <a href='https://pdal.io/index.html'>PDAL docs</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIDAR files typically come in huge sets of tiled files, so to keep things organized, we'll start by creating a directory called 'laz' to contain all of our raw original data, and an output directory called 'csv' for outputs.  (Keep in mind that our intermediate CSVs here could get big quick, and that you'll want them local to your MapD server instance if possible for speed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T01:00:08.611566Z",
     "start_time": "2018-07-12T01:00:08.596486Z"
    }
   },
   "outputs": [],
   "source": [
    "#where are we?\n",
    "pwd_slist = !pwd\n",
    "cwd = pwd_slist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T01:00:08.995016Z",
     "start_time": "2018-07-12T01:00:08.990268Z"
    }
   },
   "outputs": [],
   "source": [
    "# change paths here if needed, default puts everything relative to script directory\n",
    "\n",
    "input_dir = os.path.join(cwd, \"laz\") # read input files from here (change as useful)\n",
    "output_dir = os.path.join(cwd, \"csv\") # put results in csv folder parallel to our laz folder\n",
    "\n",
    "# sanity check\n",
    "if not os.path.exists(input_dir):\n",
    "    print('Cannot find input directory {}'.format(input_dir))\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    print('Cannot find output directory {}'.format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T01:00:10.325599Z",
     "start_time": "2018-07-12T01:00:10.320400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(input_dir):\n",
    "    !mkdir {input_dir}\n",
    "if not os.path.exists(output_dir):\n",
    "    !mkdir {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining LIDAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to follow along with an example set, this tutorial uses NOAA data from 2016 for Jekyll Island, Geogia.  We've put the URLs pointing to this data into a small text file called <a href='https://github.com/mapd/community-geo-demos/blob/master/mapd_jekyll_island_lidar_tiles.txt'>mapd_jekyll_island_lidar_tiles.txt</a>  <p>Download it, and place in your input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T23:08:17.902288Z",
     "start_time": "2018-07-11T23:08:17.897519Z"
    }
   },
   "outputs": [],
   "source": [
    "# change our current working directory to the inputs for simplicity\n",
    "%cd {input_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-12T05:09:16.313Z"
    }
   },
   "outputs": [],
   "source": [
    "# it should look something like this:\n",
    "!head -3 mapd_jekyll_island_lidar_tiles.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to try this on your own data, just put the LAZ files in the same location.  Go ahead and grab all of <a href='https://download.kortforsyningen.dk/content/geodataprodukter?field_korttype_tid_1=440&field_aktualitet_tid=435&field_datastruktur_tid=All&field_scheme_tid=All'>Denmark</a> if you want... \n",
    "\n",
    "Same code and ideas apply (but sorry, no Danish language support)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T23:10:52.100422Z",
     "start_time": "2018-07-11T23:10:48.034216Z"
    }
   },
   "outputs": [],
   "source": [
    "# use xargs to parallel download lidar tiles specified in the text file\n",
    "# the -P arguement is for number of processors, so adjust accordingly\n",
    "!xargs -i -P 4 wget '{}' -P {output_dir} < mapd_jekyll_island_lidar_tiles.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and test required libraries for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:58:16.975335Z",
     "start_time": "2018-07-12T00:58:16.864745Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #import pdal - not needed, since using commandline tools\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "\n",
    "    # standard system libraries\n",
    "    import os, glob\n",
    "    import sys, traceback\n",
    "\n",
    "    print('Imports successful')\n",
    "except ImportError:\n",
    "    print('Missing a required library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-12T05:24:23.918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use with due dilligence... performs Anaconda installs of any missing packages\n",
    "# otherwise reports versions of packages currently installed\n",
    "\n",
    "required_python_packages = ('pdal','geopandas', 'pymapd')\n",
    "        \n",
    "for package in required_python_packages:\n",
    "    # presumes you have Anaconda package manager installed, if not: try pip\n",
    "    conda_list = !conda list {package}\n",
    "    if package in conda_list[3]:\n",
    "        print('Found existing {} install: \\n{}'.format(package, conda_list[3]))\n",
    "    else:\n",
    "        print('Could not find package {} in Anaconda, installing... '.format(package))\n",
    "        !conda install {package}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert LIDAR into CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: CSV is used, despite its data volume because PDAL 1.7.2's binary shapefile exporter doesn't currently write attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T20:04:11.500602Z",
     "start_time": "2018-07-11T20:04:11.497791Z"
    }
   },
   "source": [
    "### Process LIDAR into CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic PDAL parameters are input and output paths, specified using -i and -o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because CSV is a verbose file format, it is useful to minimize the number of columns converted and to keep their text representation reflective of actual precision. <p> PDAL has a field specification syntax, which is [fieldname]:[decimal_places]<p>We feed that to the writers.text.order parameter, along with the option keep_unspecified=\"false\". <p>Note that the default precision is 3 decimal places.  This is wasteful for some attributes, but too small for latitude and longitude! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T23:12:05.638175Z",
     "start_time": "2018-07-11T23:12:05.631962Z"
    }
   },
   "outputs": [],
   "source": [
    "fieldspec = 'X:7,Y:7,' # keep all 7 decimal places we have of precision for longitude, latitude\n",
    "fieldspec += 'Z:2,' # our elevations only appear to have 2 places of precision\n",
    "fieldspec += 'Intensity:0,ReturnNumber:0,NumberOfReturns:0,Classification:0,' #integers or integer codes\n",
    "fieldspec += 'HeightAboveGround:2' # a newly-computed attribute, with 2 decimal places\n",
    "\n",
    "fieldspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This combination of options makes sense for most LIDAR files.  But if using your own data, you might want to try the 'PDAL info' tool to check if other interesting attributes exist, such as RGB color channel information.  This is vendor-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added three interesting and common computational filters to our converter below.  These are further described in PDAL's documentation, along with nearly a dozen others.  <ul><li>The 'smurf' filter applies a sophisticated ground-finding algorithm to distinguish ground surface points from others.  <li>The 'hag' filter then computes a 'height above ground' attribute which is very useful in distinguishing features.  <li>The Morton filter re-orders the points from acquisition flightline order to a scheme in which points earlier in the file provide a spatially-balanced sample of the whole file.  This is useful in testing and developing SQL queries in that you can 'limit' the queries easily but still get distributed data points.</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-11T23:02:22.865Z"
    }
   },
   "outputs": [],
   "source": [
    "# specify path to laz files and wildcard\n",
    "filespec = \"{}/*.laz\".format(input_dir)\n",
    "\n",
    "for counter, file in enumerate(glob.glob(filespec)):\n",
    "    outfile = file.replace('.laz','.csv')\n",
    "    outpath = outfile.replace(input_dir, output_dir)\n",
    "    print('Working on file {}: {}'.format(counter, file))\n",
    "    !pdal translate -i {file} -o {outpath} \\\n",
    "        -f filters.smrf -f filters.hag -f filters.mortonorder \\\n",
    "        --writers.text.keep_unspecified=\"false\" --writers.text.order={fieldspec} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:25:58.640429Z",
     "start_time": "2018-07-12T00:25:58.524583Z"
    }
   },
   "outputs": [],
   "source": [
    "# if this worked, your output directory should be getting big\n",
    "!ls {output_dir}/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scriptlet sequentially reads in CSV LIDAR files created by PDAL, and converts the file-level implicit row number from its index into an explicit attribute.  This is done before merging files so as to preserve the Morton numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional step:  preserve file-level Morton order attributes\n",
    "\n",
    "# read in uncompresssed csv files, add a first column with sequence number\n",
    "# named explictly as Morton, so that this persists after combining tiled csvs\n",
    "\n",
    "file_spec = \"*.csv\"\n",
    "csv_count = len(glob.glob(file_spec))\n",
    "\n",
    "for counter, file in enumerate(glob.glob(file_spec)):\n",
    "    print('Reading file {} of {}'.format(counter +1, csv_count))\n",
    "    csv = pd.read_csv(file)\n",
    "    \n",
    "    print('Computing morton index for file {} of {}'.format(counter +1, csv_count))\n",
    "    csv['Morton'] = csv.index\n",
    "    \n",
    "    print('Writing file {} of {}'.format(counter +1, csv_count))\n",
    "    csv.to_csv(file.replace('.csv','_morton.csv'),index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-12T01:31:10.654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing file 5 of 33\n",
      "Creating point geometry column\n",
      "Converting pandas dataframe to geopandas\n",
      "Computing morton index for file 5 of 33\n",
      "Writing file 5 of 33: /home/mapdadmin/demo/jekyll/csv/2016_PostMatthew_GA_17RMQ6043_morton.shp\n",
      "Importing file 6 of 33\n",
      "Creating point geometry column\n"
     ]
    }
   ],
   "source": [
    "# Optional step 2:  convert files to binary shapefile format\n",
    "# (these cannot be appended in MapD 4.0.0, but will be in next release)\n",
    "# Note: each individual shapefile is limited to 2G max size\n",
    "\n",
    "file_spec = os.path.join(output_dir, \"*_morton.csv\")\n",
    "print('Searching for CSV LIDAR files in directory {}'.format(file_spec))\n",
    "csv_count = len(glob.glob(file_spec))\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "for counter, file in enumerate(glob.glob(file_spec)):\n",
    "    print('Importing file {} of {}'.format(counter +1, csv_count))\n",
    "    csv = pd.read_csv(file)\n",
    "    \n",
    "    print('Creating point geometry column')\n",
    "    csv['geometry'] = csv.apply(lambda p: Point(p.X, p.Y), axis=1)\n",
    "    \n",
    "    print('Converting pandas dataframe to geopandas')\n",
    "    geocsv = gpd.GeoDataFrame(csv)\n",
    "\n",
    "    print('Computing morton index for file {} of {}'.format(counter +1, csv_count))\n",
    "    geocsv['Morton'] = geocsv.index\n",
    "    \n",
    "    out_file_name = file.replace('.csv','.shp')\n",
    "    print('Writing file {} of {}: {}'.format(counter +1, csv_count, out_file_name))\n",
    "    geocsv.to_file(out_file_name)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate MapD LIDAR Dataset from CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now  we have the data we need, and nothing we don't, all organized in a format that MapD 4.0 can read.  Let's use the pymapd library to import the data.  This is currently configured to use a local MapD install with default database, username and password, but these can easily be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:57:00.934823Z",
     "start_time": "2018-07-12T00:56:34.676808Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymapd\n",
    "print(pymapd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:59:01.029943Z",
     "start_time": "2018-07-12T00:58:25.626307Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymapd import connect\n",
    "\n",
    "dbname = 'mapd'\n",
    "user = 'mapd'\n",
    "host = 'localhost'\n",
    "password = 'HyperInteractive!'\n",
    "mapd_con = connect(user=user, password=password, host=host, dbname=dbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:59:26.936562Z",
     "start_time": "2018-07-12T00:59:01.067207Z"
    }
   },
   "outputs": [],
   "source": [
    "mapd_cursor = mapd_con.cursor()\n",
    "mapd_cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:59:26.990290Z",
     "start_time": "2018-07-12T00:59:26.975983Z"
    }
   },
   "outputs": [],
   "source": [
    "table_name = 'jekyll_lidar_2016v4'\n",
    "query = \"CREATE TABLE IF NOT EXISTS {} \".format(table_name)\n",
    "query += '(pid BIGINT, p GEOMETRY(POINT, 4326), z FLOAT, '\n",
    "query += 'Intensity FLOAT, ReturnNumber INTEGER, NumberOfReturns INTEGER, '\n",
    "query += 'Classification SMALLINT, Morton BIGINT); \n",
    "query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:59:27.119350Z",
     "start_time": "2018-07-12T00:59:27.082650Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys,traceback\n",
    "\n",
    "try:\n",
    "    print('Executing query: {}'.format(query))\n",
    "    results = mapd_cursor.execute(query)\n",
    "except:\n",
    "    print('Exception executing query')\n",
    "    a,b,c = sys.exc_info()\n",
    "    for d in traceback.format_exception(a,b,c) :\n",
    "       print (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-12T00:53:26.361Z"
    }
   },
   "outputs": [],
   "source": [
    "#import each file in sequence, with MapD appending\n",
    "file_spec = os.path.join(output_dir, \"*_morton.csv\")\n",
    "print('Searching for CSV LIDAR files in directory {}'.format(file_spec))\n",
    "csv_count = len(glob.glob(file_spec))\n",
    "csv_count\n",
    "\n",
    "for counter, file in enumerate(glob.glob(file_spec)):\n",
    "    print('Importing file {} of {}'.format(counter +1, csv_count))\n",
    "    query = \"COPY {} FROM '{}'; \".format(table_name,file)\n",
    "    try:\n",
    "        print('Executing query: {}'.format(query))\n",
    "        results = mapd_cursor.execute(query)\n",
    "    except:\n",
    "        print('Exception executing query')\n",
    "        a,b,c = sys.exc_info()\n",
    "        for d in traceback.format_exception(a,b,c) :\n",
    "           print (d)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up intermediate files here if desired\n",
    "# rm -rf {output_dir}/*.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
